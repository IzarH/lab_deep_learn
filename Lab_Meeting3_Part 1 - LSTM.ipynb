{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Amazon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import bz2\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator \n",
    "from torchtext.data.utils import get_tokenizer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read reviews files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open compressed files contacting train and test sentences from amazon reviews\n",
    "train_file = bz2.BZ2File('./train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('./test.ft.txt.bz2')\n",
    "\n",
    "# Read the content of the files. These files can be downloaded form:\n",
    "# https://www.kaggle.com/datasets/bittlingmayer/amazonreviews\n",
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()\n",
    "\n",
    "# Cut the datasets to the desired lengths and decode them\n",
    "num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
    "num_test = 400000  # Using 400,000 reviews from the test set (the entire set)\n",
    "train_set = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_set = [x.decode('utf-8') for x in test_file[:num_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 A FIVE STAR BOOK: I just finished reading Whisper of the Wicked saints. I fell in love with the caracters. I expected an average romance read, but instead I found one of my favorite books of all time. Just when I thought I could predict the outcome I was shocked ! The writting was so descriptive that my heart broke when Julia's did and I felt as if I was there with them instead of just a distant reader. If you are a lover of romance novels then this is a must read. Don't let the cover fool you this book is spectacular!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Read some reviews from the dataset using ind with different values\n",
    "# Which label (1 or 2) is for positive review and which is for negative?\n",
    "ind = 8\n",
    "print(train_set[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answer\n",
    "Which label (1 or 2) is for positive review and which is for negative?\n",
    "\n",
    "2 is a positive review label, 1 is for negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to split the labels form the sentences and prepare the sentences for tokenization, e.g., replace URLs with a URL token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(dataset):\n",
    "\n",
    "    # Extracting labels from sentences\n",
    "    labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in dataset]\n",
    "    sentences = [x.split(' ', 1)[1][:-1].lower() for x in dataset] # note the transformation for lowercase \n",
    "\n",
    "    # Some simple cleaning of data\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = re.sub('\\d','0', sentences[i])\n",
    "\n",
    "    # Modify URLs to <url>\n",
    "    for i in range(len(sentences)):\n",
    "        if 'www.' in sentences[i] or 'http:' in sentences[i] or 'https:' in sentences[i] or '.com' in sentences[i]:\n",
    "            sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", sentences[i])\n",
    "    \n",
    "    return labels, sentences\n",
    "\n",
    "\n",
    "train_labels, train_sentences = pre_processing(train_set)\n",
    "test_labels, test_sentences = pre_processing(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing the sentences we tokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%--------Original sentence--------%\n",
      "stuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^\n",
      "\n",
      "%--------Tokenized sentence--------%\n",
      "['stuning', 'even', 'for', 'the', 'non-gamer', 'this', 'sound', 'track', 'was', 'beautiful', '!', 'it', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'i', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', '.', 'game', 'music', '!', 'i', 'have', 'played', 'the', 'game', 'chrono', 'cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'i', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'it', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', '.', 'it', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get torchtext tokenizer for basic english\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Choose sentences to see their tokenization\n",
    "ind = 0\n",
    "print('%--------Original sentence--------%')\n",
    "print(train_sentences[ind])\n",
    "print('\\n%--------Tokenized sentence--------%')\n",
    "print(tokenizer(train_sentences[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build a dictionary containing words/tokens that we have in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  50000\n"
     ]
    }
   ],
   "source": [
    "# Set the maximal number of distinct tokens in the vocabulary\n",
    "max_num_tokens = int(5e4)\n",
    "\n",
    "# Define an iterator that yields the tokens\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        \n",
    "        # TODO: Tokenize the text using the tokenizer\n",
    "        tokenized_text = tokenizer(text)\n",
    "\n",
    "        yield tokenized_text\n",
    "\n",
    "\n",
    "# Define a function that build a vocabulary from dataset\n",
    "def get_vocab(train_datapipe, _max_tokens=None): \n",
    "    vocab = build_vocab_from_iterator(yield_tokens(train_datapipe), min_freq=2, specials=[\"<pad>\",\"<unk>\"], max_tokens=_max_tokens)\n",
    "\n",
    "    # TODO: Set the default index of the vocabulary to the index of <unk>\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Get the vocabulary for the Amazon Reviews training set\n",
    "vocab = get_vocab(iter(train_sentences), max_num_tokens)\n",
    "\n",
    "print('Size of the vocabulary: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the token what: 54\n",
      "the token day: 224\n",
      "the token yellow: 2514\n",
      "the token perpetual: 16460\n"
     ]
    }
   ],
   "source": [
    "# TODO: See what is the index for the following tokens in the vocabulary: 'what', 'day', 'yellow', 'perpetual'\n",
    "tokens = ['what', 'day', 'yellow', 'perpetual']\n",
    "for token in tokens:\n",
    "    print(\"the token \"+token+\":\", vocab[token])\n",
    "# At what order the dictionary is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answer \n",
    "the words are in order of the number of times seen, meaning the more they apeared in the reviews the lower index they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token number 0 is: <pad>\n",
      "token number 1 is: <unk>\n",
      "token number 2 is: .\n",
      "token number 3 is: the\n",
      "token number 4 is: ,\n",
      "token number 5 is: i\n",
      "token number 6 is: and\n",
      "token number 7 is: a\n",
      "token number 8 is: to\n",
      "token number 9 is: it\n"
     ]
    }
   ],
   "source": [
    "# TODO: see what is the tokens for the first 10 indices\n",
    "for ind in range(10):\n",
    "    print(\"token number \" +str(ind)+\" is:\", vocab.lookup_token(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special tokens:\n",
      "<pad>:  0\n",
      "<unk>:  1\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "print(\"special tokens:\")\n",
    "print(\"<pad>: \", vocab['<pad>'])\n",
    "print(\"<unk>: \", vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object yield_tokens at 0x7f7ce31d5540>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dictionary, we will set the final tokenization of the training set and the test set. Note that now we will use the indices of the tokens rather the tokens themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes a list of sentences and a vocabulary and returns the indices of the tokens in the vocabulary\n",
    "def get_inds_from_vocab(sentences_list, vocab):\n",
    "\n",
    "    # Initialize the list of tokenized sentences\n",
    "    tokenized_sentences_list = []\n",
    "\n",
    "    # For each sentence in the list\n",
    "    for sentence in sentences_list:\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        tokenized_sentence = tokenizer(sentence)\n",
    "\n",
    "        # TODO: Get the indices for the tokenized sentence using the forward method of the vocabulary\n",
    "        tokenized_sentence_inds = vocab.forward(tokenized_sentence)\n",
    "        \n",
    "        # Append the indices to the list\n",
    "        tokenized_sentences_list.append(tokenized_sentence_inds)\n",
    "\n",
    "    return tokenized_sentences_list\n",
    "\n",
    "# Get the tokens indices for the training set and the test set\n",
    "train_tokenized_sentences_inds = get_inds_from_vocab(train_sentences, vocab)\n",
    "test_tokenized_sentences_inds = get_inds_from_vocab(test_sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we tokenized the corpus, we set the sentences to a fixed length either by cutting long sentences or padding short sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len] # padding from the left so that we will sample the last value of the sentence\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "# TODO: Use the function pad_input to pad the data sets\n",
    "train_reviews = pad_input(train_tokenized_sentences_inds, seq_len=seq_len)\n",
    "test_reviews = pad_input(test_tokenized_sentences_inds, seq_len=seq_len)\n",
    "\n",
    "\n",
    "# Converting our labels into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "split_id = int(split_frac * len(test_reviews))\n",
    "\n",
    "# TODO: split the test data to validation (inds [:split_id]) and test (inds [split_id:])\n",
    "val_reviews, test_reviews = test_reviews[:split_id] , test_reviews[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id] , test_labels[split_id:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "epochs = 1\n",
    "counter = 0\n",
    "print_every = 250\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "lr = 0.005\n",
    "vocab_size = len(vocab) + 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_reviews), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_reviews), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_reviews), torch.from_numpy(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_data, num_workers=4, persistent_workers=True, shuffle=True, batch_size=batch_size, drop_last=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, num_workers=4, persistent_workers=True, shuffle=False, batch_size=6*batch_size, drop_last=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=6*batch_size, drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # TODO: Create an embedding layer form torch.nn of size vocab_size with dimension embedding_dim and set the padding_idx properly\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # TODO: Create an LSTM from torch.nn with n_layers and the parameters above. Note that batch_first should be set to true\n",
    "        self.lstm = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # Final linear layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Recast to type long\n",
    "        x = x.long()\n",
    "\n",
    "        # TODO: Get the embeddings of the current batch\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        # TODO: Process the batch using LSTM, here we can discard the returned hidden state\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        # Apply dropout\n",
    "        out = self.dropout(lstm_out)\n",
    "\n",
    "        # Apply the last layer for classification \n",
    "        out = self.fc(out).squeeze()\n",
    "\n",
    "        return out[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 2250... Loss: 0.180460... Val Loss: 0.209140... Val accuracy: 91.786%\n",
      "Validation loss decreased (0.683126 --> 0.209140).                       Saving model ...\n",
      "Epoch: 1/1... Step: 2500... Loss: 0.171479... Val Loss: 0.181286... Val accuracy: 92.955%\n",
      "Validation loss decreased (0.209140 --> 0.181286).                       Saving model ...\n",
      "Epoch: 1/1... Step: 2750... Loss: 0.183301... Val Loss: 0.174152... Val accuracy: 93.285%\n",
      "Validation loss decreased (0.181286 --> 0.174152).                       Saving model ...\n",
      "Epoch: 1/1... Step: 3000... Loss: 0.184490... Val Loss: 0.167213... Val accuracy: 93.659%\n",
      "Validation loss decreased (0.174152 --> 0.167213).                       Saving model ...\n",
      "Epoch: 1/1... Step: 3250... Loss: 0.133886... Val Loss: 0.164325... Val accuracy: 93.770%\n",
      "Validation loss decreased (0.167213 --> 0.164325).                       Saving model ...\n",
      "Epoch: 1/1... Step: 3500... Loss: 0.149206... Val Loss: 0.155926... Val accuracy: 94.115%\n",
      "Validation loss decreased (0.164325 --> 0.155926).                       Saving model ...\n",
      "Epoch: 1/1... Step: 3750... Loss: 0.187022... Val Loss: 0.160028... Val accuracy: 93.915%\n",
      "Epoch: 1/1... Step: 4000... Loss: 0.153649... Val Loss: 0.175928... Val accuracy: 93.310%\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                model.zero_grad()\n",
    "                num_correct = 0\n",
    "                for inp, lab in val_loader:\n",
    "                    inp, lab = inp.to(device), lab.to(device)\n",
    "                    out = model(inp)\n",
    "                    val_loss = criterion(out.squeeze(), lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    pred = (out.squeeze()>0).int()\n",
    "                    correct_tensor = pred.eq(lab.float().view_as(pred))\n",
    "                    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "                    num_correct += np.sum(correct)\n",
    "\n",
    "            val_acc = num_correct/len(val_loader.dataset)\n",
    "            model.train()\n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}...\".format(np.mean(val_losses)),\n",
    "                  \"Val accuracy: {:.3f}%\".format(val_acc*100))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './best_model.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).\\\n",
    "                       Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.152\n",
      "Test accuracy: 94.343%\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./best_model.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        pred = (output.squeeze()>0).int()\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
