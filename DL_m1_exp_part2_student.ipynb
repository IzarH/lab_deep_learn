{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils as utils\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import os\n",
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "import mat73\n",
        "from auxilery_functions_and_classes import *\n",
        "matplotlib.use('Agg')\n",
        "#%matplotlib inline\n",
        "%matplotlib tk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Deep Convolutional Network (using VGG13), Classifing CIFAR10\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Complete all the missing code until the Transfer Learning section.\n",
        "2. Run the code. Do you get a good performance? What is the problem in the process?\n",
        "3. Start your journey for the best performance with the scheduler. Try to improve it. Document your experiments.\n",
        "4. Now you will try adding Weight Decay. Try three numbers in the range [0.0005 - 0.005]. Document your experiments.\n",
        "5. The next step will be using Dropout. Try small probability values [0 - 0.25]. Document your experiments.\n",
        "6. The last addition will be augmentations. Be careful! try one at a time, and only combine two or more when you see improvments. Document your experiments.\n",
        "7. What is the best performance that you got?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Setting fixed seeds\n",
        "seed = 0 \n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "torch.backends.cudnn.benchmark=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining a Params object for this part\n",
        "hparams = Hyper_Params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the augmentations cell. You will use this later in this assignment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "augmentations = [#transforms.RandomHorizontalFlip(p=0.5), \n",
        "                    transforms.RandomVerticalFlip(p=0.5), \n",
        "                    transforms.RandomRotation(degrees=(-10,10)), \n",
        "                    #transforms.RandomPerspective(distortion_scale=0.5,p=0.5), \n",
        "                    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1, 1.0)), \n",
        "                    transforms.ColorJitter(brightness=(0.25,0.75), contrast=(0.25,0.75)), \n",
        "                    #transforms.RandomAffine(scale=(.9, 1.1)) \n",
        "                    ] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "means: [0.491, 0.482, 0.447] standard diviation: [0.247, 0.243, 0.262]\n"
          ]
        }
      ],
      "source": [
        "# Creating a Cifar class from cifar10.mat\n",
        "train = torchvision.datasets.CIFAR10(root=\"cifar10\",train=True,download=True)\n",
        "test = torchvision.datasets.CIFAR10(root=\"cifar10\",train=False,download=True)\n",
        "train_data = torch.tensor(train.data).to(float())/255\n",
        "test_data=torch.tensor(test.data).to(float())/255\n",
        "mean_imn = torch.mean(train_data, dim=(0, 1, 2))\n",
        "std_imn = torch.std(train_data, dim=(0, 1, 2))\n",
        "print(\"means:\",[round(value, 3) for value in mean_imn.tolist()],\"standard diviation:\",[round(value, 3) for value in std_imn.tolist()]) \n",
        "\n",
        "\n",
        "# TODO: Define the required transforms for the train and the test sets: convert the images to tensors and do normalization \n",
        "hparams.train_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean_imn, std_imn)]+augmentations) \n",
        "hparams.test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean_imn, std_imn)]) \n",
        "class_dict = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Apply transformations\n",
        "train.transform = hparams.train_transform \n",
        "test.transform = hparams.test_transform "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Defining hyper-parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Set the learning rate (try around 0.03) and the momentum (start with 0.9) \n",
        "hparams.lr =  0.03 \n",
        "hparams.momentum = 0.9 # in the SGDM algorithm, we use momentum to accelrate convergence by adding a fraction of the previous gradient step, thus overcome local minima and oscillations. \n",
        "# TODO: Define the dropout probability (0.0 at first) \n",
        "hparams.dropout_probability = 0.15\n",
        "\n",
        "hparams.epochs = 12 \n",
        "hparams.batch_size = 250\n",
        "\n",
        "hparams.scheduler_step_size = 8 # the number of epochs between each scheduler step \n",
        "hparams.scheduler_factor = 0.3 # the factor by which the scheduler changes the learning rate \n",
        "hparams.iter_break = 100 # number of iterations between validation steps\n",
        "hparams.space = 150 # for visualisation\n",
        "hparams.lines = 200 # for visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining a VGG block\n",
        "class VGGBlock(nn.Module):\n",
        "   def __init__(self,input_size,output_size):\n",
        "      super(VGGBlock,self).__init__()\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      self.Block = nn.Sequential(torch.nn.Conv2d(in_channels=input_size,out_channels=output_size, padding=1, kernel_size=(3,3), padding_mode=\"zeros\"), # TODO: complete a convolution layer with kernel size 3x3 and padding size of 1 \n",
        "                                 torch.nn.BatchNorm2d(output_size), # TODO: complete a batch normalization layer \n",
        "                                 torch.nn.ReLU(inplace=True), # TODO: complete an activation layer \n",
        "                                 torch.nn.Conv2d(in_channels=output_size,out_channels=output_size, padding=1, kernel_size=(3,3), padding_mode=\"zeros\"), # TODO: complete a convolution layer with kernel size 3x3 and padding size of 1 \n",
        "                                 torch.nn.BatchNorm2d(output_size), # TODO: complete a batch normalization layer \n",
        "                                 torch.nn.ReLU(inplace=True), # TODO: complete an activation layer\n",
        "                                 torch.nn.MaxPool2d((2,2))  # TODO: complete a max pooling layer with kernal size 2 and stride 2 \n",
        "      ) \n",
        " \n",
        "      \n",
        "   def forward(self,x):\n",
        "       return self.Block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining VGG13 using VGG Blocks\n",
        "class VGG13(nn.Module):\n",
        "    def __init__(self,input_size:tuple,channels:list,output_size:int,p=0):\n",
        "      super(VGG13,self).__init__()\n",
        "      self.vggblocks = nn.Sequential(VGGBlock(input_size[0],channels[0]), # 16x16\n",
        "                                      VGGBlock(channels[0],channels[1]),  # 8x8\n",
        "                                      VGGBlock(channels[1],channels[2]),  # 4x4\n",
        "                                      VGGBlock(channels[2],channels[3]),  # 2x2\n",
        "                                      VGGBlock(channels[3],channels[4]))  # 1x1\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      # TODO: Define a dropout layer with probability p \n",
        "      self.dropout = torch.nn.Dropout(p) \n",
        "      # TODO: Define the final linear layer \n",
        "      self.linear = torch.nn.Linear(in_features=channels[4]*1, out_features=output_size) \n",
        "      # TODO: Define a flatten layer \n",
        "      self.flatten = torch.nn.Flatten()\n",
        "     \n",
        "      \n",
        "    def forward(self,x):\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # TODO: Complete the vggblocks sequence \n",
        "      z1 = self.vggblocks(x) \n",
        "      # TODO: Complete the dropout layer \n",
        "      z2 = self.dropout(z1) \n",
        "      # TODO: Complete the flatten of z2 \n",
        "      z3 = self.flatten(z2) \n",
        "      # TODO: Complete the final linear layer \n",
        "      out = self.linear(z3) \n",
        "    \n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams.weight_decay = 0.0005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the model, optimizer, loss criterion and Scheduler and Data loaders\n",
        "model = VGG13(input_size=(3,32,32),channels=[64,128,256,512,512],output_size=10,p=hparams.dropout_probability).to(device) \n",
        "\n",
        "# TODO: Define the optimizer using sgd with momentum. Add weight decay here \n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=hparams.lr, momentum= hparams.momentum, weight_decay=hparams.weight_decay) \n",
        "\n",
        "# TODO: Define the criterion using cross entropy \n",
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "# TODO: Define the learning rate scheduler of type step \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=hparams.scheduler_step_size, gamma=hparams.scheduler_factor)\n",
        "\n",
        "# TODO: Define dataloaders (from torch) for the train and test sets, set the num_workers to 16 and use shuffle for training \n",
        "train_loader = torch.utils.data.DataLoader(train, hparams.batch_size, shuffle=True, num_workers=16)\n",
        "test_loader = torch.utils.data.DataLoader(test, hparams.batch_size, shuffle=False, num_workers=16) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(test_loader,model,criterion,acc_function):\n",
        "    with torch.no_grad():\n",
        "        loss = 0\n",
        "        acc = 0\n",
        "        for i,batch in enumerate(test_loader):\n",
        "            data,target = batch\n",
        "            output = model(data.to(device))\n",
        "            batch_loss = criterion(output,target.to(device))\n",
        "            batch_acc = acc_function(output,target.to(device))\n",
        "            loss+= batch_loss\n",
        "            acc+=batch_acc\n",
        "        return loss.item()/len(test_loader),acc/len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_with_scheduler(hparams,train_loader:DataLoader,test_loader,model:nn.Module,optimizer:torch.optim,scheduler:torch.optim.lr_scheduler,criterion,acc_function):\n",
        "      hparams.fig, (hparams.ax1, hparams.ax2) = plt.subplots(2, 1, figsize=(15, 9))\n",
        "      print_preformance_grid(Flag=True)\n",
        "      \n",
        "      for epoch in range(hparams.epochs):\n",
        "            \n",
        "            model.train()\n",
        "            hparams.epoch_accuracy_train = np.zeros(len(train_loader))\n",
        "            hparams.epoch_loss_train = np.zeros(len(train_loader))\n",
        "      \n",
        "            for i,batch in enumerate(train_loader):\n",
        "\n",
        "                # TODO: reset the optimizer gradient \n",
        "                optimizer.zero_grad() \n",
        "                data,target = batch\n",
        "                target = target.reshape(target.shape[0]).to(device)\n",
        "                output = model(data.to(device))\n",
        "\n",
        "                # TODO: Use the criterion on the output and target to get the train loss \n",
        "                loss = criterion(output,target)\n",
        "\n",
        "                # TODO: Complete the accuracy calculation \n",
        "                accuracy = acc_function(output, target) \n",
        "\n",
        "\n",
        "                # TODO: complete the backward \n",
        "                loss.backward()\n",
        "\n",
        "                # TODO: complete the algorithm step \n",
        "                optimizer.step()\n",
        "               \n",
        "                hparams.epoch_accuracy_train[i] = accuracy\n",
        "                hparams.epoch_loss_train[i] = loss.item()\n",
        "                \n",
        "                if (i==0 and epoch ==0) or (i+1) % hparams.iter_break==0:\n",
        "                    torch.cuda.empty_cache()\n",
        "                    model.eval()\n",
        "                    test_loss,test_accuracy = evaluate(test_loader,model,criterion,acc_function)                  \n",
        "                    print_performance(epoch,i,hparams,test_loss,test_accuracy)     \n",
        "                    model.train()\n",
        "\n",
        "            # TODO: complete the scheduler step \n",
        "            scheduler.step() \n",
        "\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch | Batch |      Train Loss      | Train Accuracy | Test Loss | Test Accuracy |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   0   |   0   |         2.6          |      0.1       |    2.3    |      0.1      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   1   |  100  |         2.67         |      0.12      |   38.97   |     0.11      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   1   |  200  |         2.45         |      0.15      |   55.22   |      0.1      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   2   |  100  |         2.13         |      0.2       |   13.12   |      0.1      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   2   |  200  |         2.1          |      0.21      |   13.38   |      0.1      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   3   |  100  |         2.02         |      0.24      |   2.75    |     0.18      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   3   |  200  |         1.99         |      0.25      |    3.9    |     0.17      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   4   |  100  |         1.91         |      0.29      |   4.26    |     0.14      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   4   |  200  |         1.9          |      0.29      |   2.51    |     0.26      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   5   |  100  |         1.83         |      0.32      |   6.25    |     0.24      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   5   |  200  |         1.8          |      0.34      |   3.03    |     0.26      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   6   |  100  |         1.7          |      0.38      |    2.4    |     0.29      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   6   |  200  |         1.67         |      0.39      |   2.27    |     0.34      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   7   |  100  |         1.58         |      0.43      |   3.82    |     0.34      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   7   |  200  |         1.55         |      0.44      |   2.38    |     0.36      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   8   |  100  |         1.47         |      0.47      |   1.91    |     0.41      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   8   |  200  |         1.45         |      0.47      |    2.4    |     0.39      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   9   |  100  |         1.31         |      0.53      |   1.78    |     0.43      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|   9   |  200  |         1.29         |      0.53      |   1.68    |     0.48      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|  10   |  100  |         1.25         |      0.55      |   1.88    |     0.45      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|  10   |  200  |         1.23         |      0.55      |   1.87    |     0.44      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|  11   |  100  |         1.21         |      0.56      |   1.76    |     0.47      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|  11   |  200  |         1.2          |      0.57      |   2.04    |     0.44      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|  12   |  100  |         1.16         |      0.58      |   1.66    |      0.5      |\n",
            "-------------------------------------------------------------------------------------\n",
            "|  12   |  200  |         1.17         |      0.58      |   1.79    |     0.48      |\n",
            "-------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "train_with_scheduler(hparams=hparams,train_loader=train_loader,test_loader=test_loader,model=model,optimizer=optimizer,scheduler=scheduler,criterion=criterion,acc_function=multi_class_accuracy)# to full writ\n",
        "present_confusion_matrix(model,test_loader,10,class_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transfer Learning using VGG16, on Desserts dataset\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. No code to complete in this section. Just run it!\n",
        "2. You do need to improve the performance though... Use what you learned and see where you get. Document your experiments.\n",
        "3. What is your best result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Setting fixed seeds\n",
        "seed = 6 \n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "torch.backends.cudnn.benchmark=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Defining hyper-parameters\n",
        "hparams = Hyper_Params()\n",
        "\n",
        "output_size = 6 # Number of Classes\n",
        "hparams.lr =  0.00005\n",
        "hparams.epochs = 12\n",
        "hparams.scheduler_step_size = 6\n",
        "hparams.scheduler_factor = 0.2\n",
        "hparams.batch_size = 64\n",
        "hparams.space=50\n",
        "hparams.iter_break = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a desserts class from desserts.mat\n",
        "mat_data = mat73.loadmat('desserts.mat') \n",
        "train_data = torch.tensor(mat_data['XTrain'],dtype=torch.float32).permute(3,2,0,1)/255\n",
        "train_labels = torch.tensor(mat_data['YTrain'])\n",
        "test_data = torch.tensor(mat_data['XTest'],dtype=torch.float32).permute(3,2,0,1)/255\n",
        "test_labels = torch.tensor(mat_data['YTest'])\n",
        "\n",
        "class CustomDessertsDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.data[index]\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a training set and print it\n",
        "train_set = CustomDessertsDataset(train_data,train_labels)\n",
        "print_deserts(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'imagenet_classes.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m test \u001b[38;5;241m=\u001b[39m CustomDessertsDataset(test_data,test_labels,transform\u001b[38;5;241m=\u001b[39mhparams\u001b[38;5;241m.\u001b[39mtrain_transform)\n\u001b[1;32m     12\u001b[0m train_loader,test_loader \u001b[38;5;241m=\u001b[39m prepare_dataloaders(train,test,hparams\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdisplay_mislabaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmean_imn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstd_imn\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Deep Learning experiment/Meeting 1/auxilery_functions_and_classes.py:384\u001b[0m, in \u001b[0;36mdisplay_mislabaled\u001b[0;34m(model, train_loader, mean, std)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_mislabaled\u001b[39m(model,train_loader,mean,std):    \n\u001b[1;32m    383\u001b[0m     imagenet_classes\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimagenet_classes.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m    386\u001b[0m             (key, val) \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imagenet_classes.txt'"
          ]
        }
      ],
      "source": [
        "# Demonstrating that Vgg16 trained on imagnet brings nonsense results (what happens without transfer learning)\n",
        "model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Normaize dataset with IMAGENET statistics\n",
        "mean_imn = [0.485, 0.456, 0.406]\n",
        "std_imn = [0.229, 0.224, 0.225]\n",
        "hparams.train_transform = transforms.Compose([transforms.Normalize(mean_imn,std_imn)])\n",
        "\n",
        "# Create train and test sets\n",
        "train = CustomDessertsDataset(train_data,train_labels,transform=hparams.train_transform)\n",
        "test = CustomDessertsDataset(test_data,test_labels,transform=hparams.train_transform)\n",
        "train_loader,test_loader = prepare_dataloaders(train,test,hparams.batch_size)\n",
        "\n",
        "display_mislabaled(model,train_loader,mean_imn,std_imn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Freezing the model weights, with the exeption of the last x layers\n",
        "print(\"Learnable parameters before and afetr freeze:\")\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "model = layers_to_optimize(model, 2,p1=0.5,output_size=6).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining Criterion, optimizer, Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer= optim.Adam(list(p for p in model.parameters() if p.requires_grad) , lr=hparams.lr)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=hparams.scheduler_step_size, gamma=hparams.scheduler_factor,verbose=False)\n",
        "hparams.lines = len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Augmentations cell, as before. Set augmentations and then use them.\n",
        "hparams.test_transform = transforms.Compose([transforms.Normalize(mean_imn,std_imn)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train and test sets\n",
        "train = CustomDessertsDataset(train_data,train_labels,transform=hparams.train_transform)\n",
        "test = CustomDessertsDataset(test_data,test_labels,transform=hparams.test_transform)\n",
        "train_loader,test_loader = prepare_dataloaders(train,test,hparams.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "train_with_scheduler(hparams,train_loader,test_loader,model,optimizer,scheduler,criterion,multi_class_accuracy)\n",
        "present_confusion_matrix(model,test_loader,6,{0:\"canoli\", 1:\"moose\", 2:\"churros\",3:\"creme brule\", 4:\"cupcake\", 5:\"doughnuts\" })"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML_py",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
